{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8fa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1110f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module is used for running zero shot classification with multiple GPUs\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "def candidate_labels_to_columns(candidate_labels: list, sort: bool = True) -> list:\n",
    "    \"\"\"Convert candidate labels to column names\n",
    "\n",
    "    Args:\n",
    "        candidate_labels (list): list of candidate labels\n",
    "\n",
    "    Returns: list of column names\n",
    "    \"\"\"\n",
    "    zero_shot_columns = [\n",
    "        label.replace(\" \", \"_\").replace(\"/\", \"_\").lower() for label in candidate_labels\n",
    "    ]  # replace spaces with underscores\n",
    "    if sort:\n",
    "        zero_shot_columns.sort()  # sort columns\n",
    "    return zero_shot_columns\n",
    "\n",
    "\n",
    "class model_inference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_docs,\n",
    "        candidate_labels,\n",
    "        model,\n",
    "        tokeniser,\n",
    "        multi_label=True,\n",
    "        batch_size=4,\n",
    "        seed_value=42,\n",
    "        gpu_id=-1,\n",
    "        text_column_name=\"zero_shot_text\"\n",
    "    ):\n",
    "        self.text_docs = text_docs\n",
    "        self.candidate_labels = candidate_labels\n",
    "        self.multi_label = multi_label\n",
    "        self.batch_size = batch_size\n",
    "        self.seed_value = seed_value\n",
    "        self.model = model\n",
    "        self.tokenizer = tokeniser\n",
    "        self.gpu_id = gpu_id\n",
    "        self.text_column_name = text_column_name\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model into memory\n",
    "\n",
    "        Returns:\n",
    "            transformers.pipelines.ZeroShotClassificationPipeline: zero shot learning model\n",
    "        \"\"\"\n",
    "        set_seed(self.seed_value)\n",
    "        classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            framework=\"pt\",\n",
    "            device=self.gpu_id,\n",
    "        )\n",
    "        return classifier\n",
    "\n",
    "    def _split_data_batches(self, text_docs: list) -> list:\n",
    "        \"\"\"Split data into batches of a specified size\n",
    "\n",
    "        Args:\n",
    "            text_docs (list): list of text documents of type str\n",
    "\n",
    "        Returns:\n",
    "            list: list of np.array containting text documents\n",
    "        \"\"\"\n",
    "        data_batches = np.array_split(\n",
    "            text_docs,\n",
    "            math.ceil(len(text_docs) / self.batch_size),\n",
    "        )\n",
    "        return data_batches\n",
    "\n",
    "    def _predict_data_batches(self, data_chunks) -> list:\n",
    "        \"\"\"Make predictions in batches\n",
    "\n",
    "        Args:\n",
    "            data_chunks (list): list of np.array containting text documents\n",
    "\n",
    "        Returns:\n",
    "            list: list of model results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        text_desc = (\n",
    "            \"Classifying with CPU\" if self.gpu_id == -1 else f\"Classifying with GPU {self.gpu_id}\"\n",
    "        )\n",
    "        for data in tqdm(\n",
    "            data_chunks,\n",
    "            total=len(data_chunks),\n",
    "            desc=text_desc,\n",
    "        ):\n",
    "            chunk_size = len(data)\n",
    "            result = self.classifier(\n",
    "                list(data), self.candidate_labels, multi_label=self.multi_label\n",
    "            )\n",
    "            results.extend([result]) if chunk_size == 1 else results.extend(result)\n",
    "        return results\n",
    "\n",
    "    def _convert_model_results_df(self, results) -> pd.DataFrame:\n",
    "        \"\"\"Convert model results into a pandas dataframe\n",
    "\n",
    "        Args:\n",
    "            results (list): list of model results\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: dataframe of model results\n",
    "        \"\"\"\n",
    "        # initialise dictionary with keys for each label and a text column\n",
    "        # all items are initialised as empty lists so that they can be appended\n",
    "        model_results = {label: [] for label in self.candidate_labels + [self.text_column_name]}\n",
    "\n",
    "        # loop through all results and add scores and input text to dictionary\n",
    "        for result in results:\n",
    "            # append input text to dictionary\n",
    "            model_results[self.text_column_name] += [result[\"sequence\"]]\n",
    "            # loop through all labels and add scores to dictionary\n",
    "            for i, label in enumerate(result[\"labels\"]):\n",
    "                model_results[label] += [result[\"scores\"][i]]\n",
    "\n",
    "        # convert dictionary to pandas dataframe\n",
    "        df = pd.DataFrame(model_results)\n",
    "        # update column names by replacing spaces with underscores\n",
    "        df.columns = candidate_labels_to_columns(df.columns, sort=False)\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run zero shot learning pipeline\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: model results\n",
    "        \"\"\"\n",
    "        # load model on GPU\n",
    "        self.classifier = self._load_model()\n",
    "        # split data into batches\n",
    "        data_chunks = self._split_data_batches(self.text_docs)\n",
    "        # label data in batches\n",
    "        results = self._predict_data_batches(data_chunks)\n",
    "        # convert results to dataframe\n",
    "        df = self._convert_model_results_df(results)\n",
    "        # sort columns\n",
    "        sorted_columns = candidate_labels_to_columns(self.candidate_labels)\n",
    "        sorted_columns.append(self.text_column_name)\n",
    "        df = df[sorted_columns]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad96131",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\n",
    "  \"data orchestration\", \n",
    "  \"container orchestration\",\n",
    "  \"data transform\", \n",
    "  \"data ingestion\",\n",
    "  \"scheduling\",\n",
    "  \"ETL jobs\", \n",
    "  \"learning\", \n",
    "  \"streaming\",\n",
    "  \"data lake\",\n",
    "  \"data lakehouse\",\n",
    "  \"data warehouse\",\n",
    "  \"data mesh\",\n",
    "  \"career\",\n",
    "  \"CI/CD\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055f3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "\n",
    "def save_zero_shot_model(model_name=\"facebook/bart-large-mnli\", model_path=\"./zero-shot-model/\"):\n",
    "    \"\"\"Save pretrained zero shot model to disk\n",
    "\n",
    "    Args:\n",
    "        model_name (str, optional): Name of sequence classification model. Defaults to \"facebook/bart-large-mnli\".\n",
    "        model_path (str, optional): Local path to save model. Defaults to \"./zero-shot-model/\".\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=model_name,\n",
    "        config=AutoConfig.from_pretrained(model_name),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "    # save model\n",
    "    classifier.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d45223",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m8/4wf1gc_13fgfhnzfz74zs2v00000gq/T/ipykernel_4253/3707225915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_zero_shot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m8/4wf1gc_13fgfhnzfz74zs2v00000gq/T/ipykernel_4253/2870662134.py\u001b[0m in \u001b[0;36msave_zero_shot_model\u001b[0;34m(model_name, model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m                     resolved_vocab_files[file_id] = cached_path(\n\u001b[0m\u001b[1;32m   1693\u001b[0m                         \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1626\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reddit/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_zero_shot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9162d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = 'post_combined_title_description'\n",
    "date_column = 'post_created_utc_date'\n",
    "incremental_interval = '3 day'\n",
    "other_columns = 'id,post_created_utc_date'\n",
    "zero_shot_model_path = './zero-shot-model/'\n",
    "ext_zero_shot_table_schema = 'ext_staging'\n",
    "seed_value = 42\n",
    "batch_size = 4\n",
    "multi_label_prediction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb13955",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_table = \"blogs.reddit_dataengineering\"\n",
    "zero_shot_table = \"blogs.reddit_zero_shot\"\n",
    "is_incremental = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9a7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = -1\n",
    "sorted_zero_shot_columns = candidate_labels_to_columns(candidate_labels)\n",
    "sorted_zero_shot_columns.append(\"zero_shot_text\")\n",
    "select_other_columns_query = \",\".join([f\"{text_table}.{col}\" for col in other_columns.split(\",\")]) if other_columns else \"\"\n",
    "incremental_query = f\"\"\"\n",
    "WHERE {date_column} >= (\n",
    "SELECT\n",
    "    max({date_column}) - interval '{incremental_interval}'\n",
    "FROM {zero_shot_table}\n",
    ")\n",
    "\"\"\" if is_incremental else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e47a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(incremental_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054837da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = f\"\"\"\n",
    "SELECT\n",
    "    {select_other_columns_query},\n",
    "    {text_table}.{text_column}\n",
    "FROM\n",
    "    {text_table}\n",
    "{incremental_query};\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ad24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "    blogs.reddit_dataengineering.id,blogs.reddit_dataengineering.post_created_utc_date,\n",
      "    blogs.reddit_dataengineering.post_combined_title_description\n",
      "FROM\n",
      "    blogs.reddit_dataengineering\n",
      ";\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ef48c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./postgres/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "372d3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_USER = os.getenv('POSTGRES_USER')\n",
    "PG_PW = os.getenv('POSTGRES_PASSWORD')\n",
    "PG_DB = os.getenv('POSTGRES_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd429c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_conn_args = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": PG_DB,\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PW,\n",
    "    \"port\": 5432\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48aa4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_conn = psycopg2.connect(**postgres_conn_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0929185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pg_conn:\n",
    "    df = pd.read_sql(Q, pg_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f61ebeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Guide to Data Versioning '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(\"post_combined_title_description\")].iloc[0].post_combined_title_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af7c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_created_utc_date</th>\n",
       "      <th>post_combined_title_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>rk84l6</td>\n",
       "      <td>2021-12-19</td>\n",
       "      <td>The Guide to Data Versioning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>ra64xg</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>The Guide to Data Versioning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id post_created_utc_date post_combined_title_description\n",
       "52   rk84l6            2021-12-19   The Guide to Data Versioning \n",
       "252  ra64xg            2021-12-06   The Guide to Data Versioning "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.post_combined_title_description == 'The Guide to Data Versioning ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cf7b9635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data orchestration',\n",
       " 'container orchestration',\n",
       " 'data transform',\n",
       " 'data ingestion',\n",
       " 'scheduling',\n",
       " 'ETL jobs',\n",
       " 'learning',\n",
       " 'streaming',\n",
       " 'data lake',\n",
       " 'data lakehouse',\n",
       " 'data warehouse',\n",
       " 'data mesh',\n",
       " 'career',\n",
       " 'CI/CD']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "00bccab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_model = model_inference(\n",
    "    text_docs=df[text_column].values,\n",
    "    candidate_labels=candidate_labels,\n",
    "    model=zero_shot_model_path,\n",
    "    tokeniser=zero_shot_model_path,\n",
    "    multi_label=multi_label_prediction,\n",
    "    batch_size=batch_size,\n",
    "    seed_value=seed_value,\n",
    "    gpu_id=gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc67f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f9ccc8c1c7407fb11b64da17d60aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying with CPU:   0%|          | 0/1913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = zero_shot_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc5e57fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>ci_cd</th>\n",
       "      <th>container_orchestration</th>\n",
       "      <th>data_ingestion</th>\n",
       "      <th>data_lake</th>\n",
       "      <th>data_lakehouse</th>\n",
       "      <th>data_mesh</th>\n",
       "      <th>data_orchestration</th>\n",
       "      <th>data_transform</th>\n",
       "      <th>data_warehouse</th>\n",
       "      <th>etl_jobs</th>\n",
       "      <th>learning</th>\n",
       "      <th>scheduling</th>\n",
       "      <th>streaming</th>\n",
       "      <th>zero_shot_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098197</td>\n",
       "      <td>0.157531</td>\n",
       "      <td>0.171678</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>0.160785</td>\n",
       "      <td>0.276192</td>\n",
       "      <td>0.376911</td>\n",
       "      <td>0.310253</td>\n",
       "      <td>0.148455</td>\n",
       "      <td>0.264665</td>\n",
       "      <td>0.022643</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.055658</td>\n",
       "      <td>0.125571</td>\n",
       "      <td>Considerations for System Design to solve Scal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.962993</td>\n",
       "      <td>0.248706</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>0.207498</td>\n",
       "      <td>0.235998</td>\n",
       "      <td>0.315218</td>\n",
       "      <td>0.311318</td>\n",
       "      <td>0.235031</td>\n",
       "      <td>0.320939</td>\n",
       "      <td>0.201492</td>\n",
       "      <td>0.112974</td>\n",
       "      <td>0.935020</td>\n",
       "      <td>0.247679</td>\n",
       "      <td>0.340568</td>\n",
       "      <td>Any advice for 20-30 years commitment to Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.878054</td>\n",
       "      <td>0.182176</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.125504</td>\n",
       "      <td>0.162288</td>\n",
       "      <td>0.145944</td>\n",
       "      <td>0.148986</td>\n",
       "      <td>0.100657</td>\n",
       "      <td>0.191071</td>\n",
       "      <td>0.106329</td>\n",
       "      <td>0.078659</td>\n",
       "      <td>0.301565</td>\n",
       "      <td>0.086557</td>\n",
       "      <td>0.295934</td>\n",
       "      <td>Is data engineering stressful? Hi everyone!\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259345</td>\n",
       "      <td>0.063874</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.346872</td>\n",
       "      <td>0.150331</td>\n",
       "      <td>0.425382</td>\n",
       "      <td>0.053838</td>\n",
       "      <td>0.697812</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>0.149373</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.209318</td>\n",
       "      <td>0.087859</td>\n",
       "      <td>0.028639</td>\n",
       "      <td>Save NumPy Arrays to CSV Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.651201</td>\n",
       "      <td>0.149031</td>\n",
       "      <td>0.023593</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.062665</td>\n",
       "      <td>0.142306</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.088188</td>\n",
       "      <td>0.144680</td>\n",
       "      <td>0.031745</td>\n",
       "      <td>0.060156</td>\n",
       "      <td>0.503379</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.147784</td>\n",
       "      <td>Is being a data engineer just a specialised so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666140</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.059496</td>\n",
       "      <td>0.488118</td>\n",
       "      <td>0.430132</td>\n",
       "      <td>0.363005</td>\n",
       "      <td>0.565051</td>\n",
       "      <td>0.529929</td>\n",
       "      <td>0.446419</td>\n",
       "      <td>0.420775</td>\n",
       "      <td>0.038153</td>\n",
       "      <td>0.703167</td>\n",
       "      <td>0.330835</td>\n",
       "      <td>0.488099</td>\n",
       "      <td>Need help to prepare for FAANG data engineer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.238368</td>\n",
       "      <td>0.209683</td>\n",
       "      <td>0.071292</td>\n",
       "      <td>0.265850</td>\n",
       "      <td>0.149031</td>\n",
       "      <td>0.149039</td>\n",
       "      <td>0.043503</td>\n",
       "      <td>0.376699</td>\n",
       "      <td>0.327314</td>\n",
       "      <td>0.131192</td>\n",
       "      <td>0.121924</td>\n",
       "      <td>0.174928</td>\n",
       "      <td>0.361687</td>\n",
       "      <td>0.213443</td>\n",
       "      <td>Anyone using python ray.io framework in produc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     career     ci_cd  container_orchestration  data_ingestion  data_lake  \\\n",
       "0  0.098197  0.157531                 0.171678        0.174216   0.160785   \n",
       "1  0.962993  0.248706                 0.080922        0.207498   0.235998   \n",
       "2  0.878054  0.182176                 0.045917        0.125504   0.162288   \n",
       "3  0.259345  0.063874                 0.015281        0.346872   0.150331   \n",
       "4  0.651201  0.149031                 0.023593        0.051948   0.062665   \n",
       "5  0.666140  0.015538                 0.059496        0.488118   0.430132   \n",
       "6  0.238368  0.209683                 0.071292        0.265850   0.149031   \n",
       "\n",
       "   data_lakehouse  data_mesh  data_orchestration  data_transform  \\\n",
       "0        0.276192   0.376911            0.310253        0.148455   \n",
       "1        0.315218   0.311318            0.235031        0.320939   \n",
       "2        0.145944   0.148986            0.100657        0.191071   \n",
       "3        0.425382   0.053838            0.697812        0.130947   \n",
       "4        0.142306   0.146887            0.088188        0.144680   \n",
       "5        0.363005   0.565051            0.529929        0.446419   \n",
       "6        0.149039   0.043503            0.376699        0.327314   \n",
       "\n",
       "   data_warehouse  etl_jobs  learning  scheduling  streaming  \\\n",
       "0        0.264665  0.022643  0.100460    0.055658   0.125571   \n",
       "1        0.201492  0.112974  0.935020    0.247679   0.340568   \n",
       "2        0.106329  0.078659  0.301565    0.086557   0.295934   \n",
       "3        0.149373  0.000651  0.209318    0.087859   0.028639   \n",
       "4        0.031745  0.060156  0.503379    0.014783   0.147784   \n",
       "5        0.420775  0.038153  0.703167    0.330835   0.488099   \n",
       "6        0.131192  0.121924  0.174928    0.361687   0.213443   \n",
       "\n",
       "                                      zero_shot_text  \n",
       "0  Considerations for System Design to solve Scal...  \n",
       "1  Any advice for 20-30 years commitment to Data ...  \n",
       "2  Is data engineering stressful? Hi everyone!\\n\\...  \n",
       "3                    Save NumPy Arrays to CSV Files   \n",
       "4  Is being a data engineer just a specialised so...  \n",
       "5  Need help to prepare for FAANG data engineer i...  \n",
       "6  Anyone using python ray.io framework in produc...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "76587d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = df_results.iloc[0].zero_shot_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e801475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=zero_shot_model_path,\n",
    "            tokenizer=zero_shot_model_path,\n",
    "            framework=\"pt\",\n",
    "            device=gpu_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f5303651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_created_utc_date</th>\n",
       "      <th>post_combined_title_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rou5l5</td>\n",
       "      <td>2021-12-26</td>\n",
       "      <td>Considerations for System Design to solve Scal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roqnd3</td>\n",
       "      <td>2021-12-26</td>\n",
       "      <td>Any advice for 20-30 years commitment to Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roq6gw</td>\n",
       "      <td>2021-12-26</td>\n",
       "      <td>Is data engineering stressful? Hi everyone!\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roh6z4</td>\n",
       "      <td>2021-12-25</td>\n",
       "      <td>Save NumPy Arrays to CSV Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rofnm0</td>\n",
       "      <td>2021-12-25</td>\n",
       "      <td>Is being a data engineer just a specialised so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>9obxqc</td>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>Iceberg: Improving The Utility Of Cloud-Native...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>9mz3eq</td>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>Flink Vs Spark | Apache Flink is successor to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7648</th>\n",
       "      <td>9mrf65</td>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>Insight Data Engineering Fellowship coming to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7649</th>\n",
       "      <td>9monpj</td>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>Fast, Scalable, and Flexible Data For Applicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7650</th>\n",
       "      <td>96cpq6</td>\n",
       "      <td>2018-08-11</td>\n",
       "      <td>Anybody using Record Format conversion in AWS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7651 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id post_created_utc_date  \\\n",
       "0     rou5l5            2021-12-26   \n",
       "1     roqnd3            2021-12-26   \n",
       "2     roq6gw            2021-12-26   \n",
       "3     roh6z4            2021-12-25   \n",
       "4     rofnm0            2021-12-25   \n",
       "...      ...                   ...   \n",
       "7646  9obxqc            2018-10-15   \n",
       "7647  9mz3eq            2018-10-10   \n",
       "7648  9mrf65            2018-10-09   \n",
       "7649  9monpj            2018-10-09   \n",
       "7650  96cpq6            2018-08-11   \n",
       "\n",
       "                        post_combined_title_description  \n",
       "0     Considerations for System Design to solve Scal...  \n",
       "1     Any advice for 20-30 years commitment to Data ...  \n",
       "2     Is data engineering stressful? Hi everyone!\\n\\...  \n",
       "3                       Save NumPy Arrays to CSV Files   \n",
       "4     Is being a data engineer just a specialised so...  \n",
       "...                                                 ...  \n",
       "7646  Iceberg: Improving The Utility Of Cloud-Native...  \n",
       "7647  Flink Vs Spark | Apache Flink is successor to ...  \n",
       "7648  Insight Data Engineering Fellowship coming to ...  \n",
       "7649  Fast, Scalable, and Flexible Data For Applicat...  \n",
       "7650  Anybody using Record Format conversion in AWS ...  \n",
       "\n",
       "[7651 rows x 3 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9f7f87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.merge(\n",
    "    df_results, left_on = \"post_combined_title_description\", right_on = \"zero_shot_text\"\n",
    ").drop(\"post_combined_title_description\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "35b3819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"a\", \"b\"] + [\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "514f210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_2 = df.iloc[0:7].join(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "708ce410",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_2 = final_df_2.drop(\"post_combined_title_description\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "43b3608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 17)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_2.iloc[0:7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "68a79e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.equals(final_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8987cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_df.compare(final_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e6b15063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e537af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
